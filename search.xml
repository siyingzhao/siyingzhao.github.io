<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Machine Learning: Logistic回归</title>
    <url>/2025/07/01/Machine-Learning-Logistic%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<p>​ 机器学习系列中逻辑回归一块的sigmoid函数和损失函数定义。</p>
<span id="more"></span>
<h2 id="sigmoid函数">Sigmoid函数</h2>
<p><span class="math display">\[
z=w^Tx+b
\]</span></p>
<p><span class="math display">\[
\sigma\left(z\right)=Sigmoid(z)=\frac{1}{1+e^{-z}}
\]</span></p>
<p>相当于把线性回归的结果从实数域映射到(0,1)上。</p>
<h2 id="损失函数成本函数">损失函数(成本函数)</h2>
<p><span class="math display">\[
J(w,b)=\frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})=-\frac{1}{m}\sum_{i=1}^m\left[y^{(i)}*\log(\hat{y}^{(i)})+(1-y^{(i)})*\log(1-\hat{y}^{(i)})\right]
\]</span></p>
<p>以往线性回归损失函数<span
class="math inline">\(L(\hat{y},y)=\frac{1}{2}(y-\hat{y})^2\)</span>的缺点是用在逻辑回归后，最低点的极值不止一个，可能在使用梯度下降接近寻找损失函数最低点时会遇到困难，所以不使用上面这种损失函数，而采用下面这种：</p>
<p><span class="math display">\[
L(\hat{y},y)=-\left[y*\log(\hat{y})+(1-y)*\log(1-\hat{y})\right]
\]</span></p>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>大二学年总结</title>
    <url>/2025/07/05/%E5%A4%A7%E4%BA%8C%E5%AD%A6%E5%B9%B4%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>​ 对在清华第二年的简单总结。</p>
<span id="more"></span>
]]></content>
      <tags>
        <tag>memory</tag>
      </tags>
  </entry>
</search>
